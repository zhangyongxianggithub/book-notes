# Flink中的API
Flink为流/批应用程序的开发提供了不同级别的抽象:
![不同级别的抽象](pic/levels_of_abstraction.svg)
- Flink API最底层的抽象是有状态实时流处理，其抽象实现是[Process Function](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/datastream/operators/process_function/)，并且**Process Function**被Flink框架集成到了DataStream API中来为我们使用，它允许用户在应用程序中自由的处理来自单流或者多流的事件，并提供具有全局一致性和容错保障的状态。此外，用户可以在此层抽象中注册事件时间（event time）和处理时间(processing time)回调方法，从而允许程序可以实现复杂计算;
- Flink API的第二层抽象是Core APIs，许多应用程序不需要使用到上述最底层抽象的API，而是可以使用Core APIs进行编程，其中包含DataStream API(应用于有界/无界数据流场景)和DataSet API(应用于有界数据集场景)2部分。Core APIs提供的流式API为数据处理提供了通用的模块组件，例如各种形式的用户自定义转换(transformations)、联接(joins)、聚合(aggregations)、窗口(windows)和状态(state)操作等，此层API中处理的数据类型在没种编程语言中都有其对应的类。Process Function这类底层抽象和DataStream API的相互集成使得用户可以选择使用更底层的抽象API来实现自己的需求，DataSet API还额外提供了一些源语，比如循环/迭代操作;
- Flink API第三层抽象是Table API，Table API是以表为中心的声明式编程API，例如在流式数据场景下，它可以表示一张正在动态改变的表，Table API遵循关系模型: 即表拥有schema，并且Table API也提供了类似关系模型中的操作，比如select、project、join、group by和aggregate等，Table API程序是以声明的方式定义应执行的逻辑操作，而不是确切的指定程序应该执行的代码，尽管Table API使用起来很简洁并且可以由各种类型的用户自定义函数扩展功能，但还是比Core API的表达能力差，此外，Table API程序在执行之前还会使用优化器中的优化规则对用户编写的表达式进行优化，表和DataStream/DataSet可以进行无缝切换，Flink允许用户在编写应用程序时将Table API与DataStream/DataSet API混合使用;
- Flink API最顶层抽象是SQL，这层抽象在语义和程序表达上都类似于Table API，但是其程序实现都是SQL查询表达式，SQL抽象与Table API抽象之间的关联是非常紧密的，并且SQL查询语句可以在Table API中定义的表上执行。

# 有状态流处理
## 状态是什么
数据流中的很多操作一次只处理一个单一的事件，比如事件解析器，也有一些操作需要记录横跨多个事件的一些信息，比如窗口计算，这些计算就叫做有状态的。有状态计算的例子:
- 当一个应用搜索具体事件之间的关系模式，状态会存储连续的事件序列;
- 当按照分/小时/天单位维度聚合事件时，状态存储中间聚合状态;
- 当在数据流上训练机器学习模型时，状态需要保存模型参数;
- 当需要管理历史数据时，状态可以有效的访问历史事件。
Flink需要知道状态的存在以便通过checkpoints与savepoint机制实现容错，状态还支持Flink应用的扩缩容，也就是Flink会在并行的实例间重新分发状态。[可查询状态](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/datastream/fault-tolerance/queryable_state/)允许你从Flink外部访问状态。使用状态机制前，你需要了解[Flink's state backends](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/ops/state/state_backends/)，Flink提供了多种state backends。
## Keyed State
分区状态可以被认为一种内嵌的key/value存储，有状态的操作读取流时，这些状态是在流上严格的分区并在在这些分区上分布的，因而，只允许在分区流上访问这些键值存储，比如，在一个keyed/partitioned数据交换操作后，访问键值存储只允许访问与当前的事件key相关联的value，对齐流的key与状态可以确保所有的状态更新都是本地操作，不需要事务的支持就可以保证数据一致性。这种对齐，也允许flink重新分发状态或者调整流的分区设置。
![分区状态](pic/state_partitioning.svg)
分区状态也会被深入组织成key groups，key groups是一种原子单元，Flink重新分发状态时，就是重新分发这种原子单元，key groups的数量与定义的最大并行度数量一样，在执行期间，每一个分区算子并行实例工作在一个或者多个key groups的keys上。
## State Persistence
Flink组合使用**流重放**机制与**checkpointing**机制实现了容错处理，一个checkpoint就是输入流中的一个特殊标记点，这个标记点还具有流经算子时对应的状态，流式数据流可以从一个checkpoint处重放，重放会保证数据一致性(精确一次处理语义)，这是通过存储的算子的状态并且从checkpoint标记点之后重放记录实现的。checkpoint间隔需要权衡考虑，综合焦虑执行期间的容错开销与恢复时的恢复时间(需要重放的记录数)。容错机制不断绘制分布式流式数据流的快照。对于状态较小的流式应用程序，这些快照非常轻量级，可以频繁绘制而不会对性能产生太大影响。 流应用程序的状态存储在可配置的位置，通常在分布式文件系统中。如果出现程序故障(由于机器、网络或软件故障)，Flink会停止分布式数据流。然后系统重新启动算子并将它们重置为最后成功的checkpoint。输入流被重置为状态快照标记点。重新启动的并行数据流处理的任何记录都保证不会影响先前的检查点状态。
默认情况下，checkpointing时关闭的，可以参考[Checkpointing]部分获得开启/配置checkpointing机制的细节。
为了使该机制实现其完全保证，数据流source(例如message queue或者broker)需要能够将流倒回到定义的最近点。[Apache Kafka](http://kafka.apache.org/)具有这种能力，Flink的Kafka connector利用了这一点。 有关Flink connectors提供的保证的更多信息，请参阅[数据源和数据汇的容错保证](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/connectors/datastream/guarantees/)。因为Flink的checkpoints是通过分布式快照实现的，所以我们将snapshot和checkpoint互换使用。我们通常还使用术语snapshot来表示checkpoint或savepoint。
### Checkpointing
Flink容错机制的核心部分是对分布式数据流和算子状态绘制一致的快照。这些快照充当一致的checkpoints，系统可以在发生故障时回退到这些checkpoint。Flink绘制这些快照的机制在[Lightweight Asynchronous Snapshots for Distributed Dataflows](http://arxiv.org/abs/1506.08603)中有描述。它的灵感来自分布式快照的标准算法[Chandy-Lamport](http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf)，Flink专门针对执行模型做了定制。请记住，与检查点有关的所有事情都可以异步完成。检查点barriers不会以锁的方式进行，并且操作可以异步快照其状态。从Flink 1.11版本开始，可以在对齐或不对齐的情况下记录checkpoints。在本节中，我们首先描述aligned checkpoints。
1. Barriers
   Flink分布式快照中的一个核心元素是*stream barriers*，这些栅栏被注入到数据流中，作为数据流的一部分记录存在，栅栏记录不存在乱序，它们是严格的一条线上的顺序流，栅栏将数据流中的记录分为进入当前快照的记录集和进入下一个快照的记录。每个栅栏都带有快照的ID，快照包含的记录都在它前面推送。栅栏不会中断数据流，非常轻量级。来自不同快照的多个栅栏可以同时在流中，这意味着各种快照可能并发发生。
   ![栅栏](pic/stream_barriers%20(1).svg)
   Stream栅栏在数据源被注入到并行的数据流，注入快照$n$的栅栏的点（我们称之为$S_n$）是源流中快照覆盖数据的位置。例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。这个位置$S_n$被报告给checkpoint coordinator（Flink 的 JobManager）。栅栏然后流向下游，当中间算子从其所有输入流中接收到快照$n$的栅栏时，它会将快照$n$的栅栏发射到其所有输出流中。一旦数据汇算子（流式DAG的末端）从其所有输入流中接收到栅栏$n$，它就会向checkpoint coordinator ack快照$n$。 在所有数据汇都ack快照后，它被认为已完成。一旦快照$n$完成，job将不会向数据源请求任何$S_n$位置之前的记录，因为这些记录都已经通过了整个数据流的拓扑逻辑（栅栏就是设置路卡拦截的意思）。接受多个输入流的算子需要在快照栅栏上对齐输入流，下面的图片表明
   ![栅栏对齐](pic/stream_aligning%20(1).svg)
   - 只要算子从任意一个输入流中接收到栅栏$n$，它将不能继续处理流中后面的记录，直到从其他流中都接收到栅栏$n$，如果不等待，那么快照的输入流中将含有$n+1$的记录;
   - 一旦从最后一个输入流中接收到栅栏$n$，算子会将等待中的输出记录发送出去，然后发送栅栏$n$;
   - flink将状态存储为快照，开始处理后面的输入流数据;
   - 最后，算子异步的将状态写到state backend;
  请注意，所有有多个输入流的算子都需要对齐。
2. Snapshotting Operator State
   当算子包含任何形式的状态，快照将会包含状态。算子从所有输入流接收到快照栅栏后在将栅栏发射到其输出流之前的时间点快照其状态。此时，所有对状态的更新都来自栅栏之前的记录，并且没有任何更新依赖于应用栅栏之后的记录。因为快照的状态可能很大，所以它存储在可配置的*state backend*中。默认情况下，这是JobManager的内存，但对于生产使用，应配置分布式持久存储（例如 HDFS）。存储状态后，算子ack checkpoint，将快照栅栏发送到输出流中，然后继续处理后面的数据。最终快照包含:
   - 对于并行的数据源，包含流中的offset/position;
   - 对于每个算子，包含对于状态（作为快照的一部分存储）的指针
   ![快照包含的内容](pic/checkpointing.svg)
3. Recovery
   恢复的当时很简单，一旦失败，Flink选择最后的完成的checkpoint $k$执行恢复，Flink会重新部署整个分布式数据流，给每个算子分配快照$k$中的状态，数据源被重置为从快照中的指定位置开始读取输入流，如果状态是增量快照的，算子将会从最后一次全量快照的状态开始，依次应用快照的增量更新。可以参考[重启策略](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/ops/state/task_failure_recovery/#restart-strategies)获取更多的信息。
4. 未对齐的checkpointing
   checkpointing时也可以不用对齐，基本的观点是，checkpoints
5. 未对齐的恢复


