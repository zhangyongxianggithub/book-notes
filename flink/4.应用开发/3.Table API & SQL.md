[TOC]

Apache Flink有2种关系型API来做流批统一处理: Table API与SQL，Table API是用于Scala和Java语言的查询API，它可以用一种非常直观的方式来组合使用选取、过滤、join等关系算子。Flink SQL是基于Apache Calcite来实现的标准SQL。无论输入是连续的还是有界的，在2个接口中指定的查询都具有相同的语义并指定相同的结果。Table API和SQL2种API是紧密集成的以及DataStream API。你可以在这些API之间以及一些基于这些API的库之间轻松的切换。比如，你可以先用CEP从DataStream中做模式匹配，然后用Table API来分析匹配的结果，或者你可以用SQL来扫描、过滤、聚合一个批式的表，然后再跑一个Gelly图算法来处理已经预处理好的数据。
你需要将Table API作为依赖项添加到项目中，以便Table API和SQL定义数据管道。
# 概念与通用API
Table API和SQL集成在同一套API中。这套API的核心概念是Table，用作查询的输入与输出。本文介绍Table API和SQL查询程序的通用结构、如何注册Table、如何查询Table以及如何输出Table。
## Table API和SQL程序的结构
所有用于批处理和流处理的Table API和SQL程序都遵循相同的模式。下面的代码示例展示了Table API和SQL程序的通用结构:
```java
import org.apache.flink.table.api.*;
import org.apache.flink.connector.datagen.table.DataGenConnectorOptions;

// Create a TableEnvironment for batch or streaming execution.
// See the "Create a TableEnvironment" section for details.
TableEnvironment tableEnv = TableEnvironment.create(/*…*/);

// Create a source table
tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen")
    .schema(Schema.newBuilder()
      .column("f0", DataTypes.STRING())
      .build())
    .option(DataGenConnectorOptions.ROWS_PER_SECOND, 100L)
    .build());

// Create a sink table (using SQL DDL)
tableEnv.executeSql("CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable (EXCLUDING OPTIONS) ");

// Create a Table object from a Table API query
Table table1 = tableEnv.from("SourceTable");

// Create a Table object from a SQL query
Table table2 = tableEnv.sqlQuery("SELECT * FROM SourceTable");

// Emit a Table API result Table to a TableSink, same for SQL result
TableResult tableResult = table1.insertInto("SinkTable").execute();
```
**注意**: Table API和SQL查询可以很容易地集成并嵌入到DataStream程序中。请参阅与[DataStream API集成](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/table/data_stream_api/)章节了解如何将DataStream与表之间相互转化。
## 创建TableEnvironment
TableEnvironment是Table API和SQL的核心概念。它负责:
- 在内部的catalog中注册Table;
- 注册外部的catalog;
- 加载可插拔模块;
- 执行SQL查询;
- 注册自定义函数(scalar、table或aggregation);
- DataStream和Table之间的转换(面向StreamTableEnvironment).

Table总是与特定的TableEnvironment绑定，不能在同一条查询中使用不同的TableEnvironment中的表，例如，对它们进行join或union操作。TableEnvironment可以通过静态方法`TableEnvironment.create()`创建。
```java
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.TableEnvironment;
EnvironmentSettings settings = EnvironmentSettings
    .newInstance()
    .inStreamingMode()
    //.inBatchMode()
    .build();
TableEnvironment tEnv = TableEnvironment.create(settings);
```
或者用户可以从现有的StreamExecutionEnvironment创建一个StreamTableEnvironment与DataStream API互操作。
```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
```
## 在Catalog中创建表
TableEnvironment维护着一个由标识符(identifier)创建的表catalog的映射。标识符由3个部分组成: catalog名称、数据库名称以及对象名称，如果catalog或者数据库没有指明，就会使用当前默认值(参见[表标识符扩展](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/table/common/#expanding-table-identifiers)章节中的例子)。Table可以是虚拟的(视图VIEWS)也可以是常规的(表TABLES)，视图VIEWS也可以从已经存在的Table中创建，一般是Table API或者SQL的查询结果。表TABLES描述的是外部数据，例如文件、数据库表或者消息队列。
### 临时表(Temporary Table)和永久表(Permanent Table)
表可以是临时的，并与单个Flink会话(session)的生命周期相关，也可以是永久的，并且在多个Flink会话和集群中可见。永久表需要catalog以维护表的元数据。一旦永久表被创建，它将对任何连接到catalog的Flink会话可见且持续存在，直至被明确删除。临时表通常保存于内存中并且仅在创建它们的flink会话持续期间存在。这些表对于其他会话是不可见的。它们不与任何catalog或者数据库绑定但可以在一个命名空间(namespace)中创建，即使它们对应的数据库被删除，临时表也不会被删除。
### 屏蔽(Shadowing)
可以使用与已存在的永久表相同的标识符去注册临时表，临时表会屏蔽永久表，并且只要临时表存在，永久表就无法访问。所有使用该标识符的查询都将作用于临时表。这可能对实验(experimentation)有用，它允许先对一个临时表进行完全相同的查询，例如只有一个子集的数据，或者数据是不确定的。一旦验证了查询的正确性，就可以对实际的生产表进行查询。
## 创建表
### 虚拟表
在SQL的术语中，Table API的对象对应于视图(虚拟表)。它封装了一个逻辑查询计划。可以通过一下方法在catalog中创建:
```java
// get a TableEnvironment
TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
// table is the result of a simple projection query 
Table projTable = tableEnv.from("X").select(...);
// register the Table projTable as table "projectedTable"
tableEnv.createTemporaryView("projectedTable", projTable);
```
**注意**: 从传统数据库系统的角度来看，Table对象与VIEW视图非常像。也就是，定义了Table的查询是没有被优化的，而且会被嵌入到另一个引用了这个注册了的Table的查询中。如果多个查询都引用了同一个注册了的Table，那么它会被内嵌到每个查询中并被执行多次，也就是说注册了的Table的结果不会被共享。
#### Connector Tables
另外一个方式去创建TABLE是通过connector声明，Connector描述了存储表数据的外部系统。存储系统例如Apache Kafka或者常规的文件系统都可以通过这种方式来声明。这样的表可以直接通过Table API直接创建也可以直接通过SQL DDL创建。
```sql
// Using table descriptors
final TableDescriptor sourceDescriptor = TableDescriptor.forConnector("datagen")
    .schema(Schema.newBuilder()
    .column("f0", DataTypes.STRING())
    .build())
    .option(DataGenConnectorOptions.ROWS_PER_SECOND, 100L)
    .build();
tableEnv.createTable("SourceTableA", sourceDescriptor);
tableEnv.createTemporaryTable("SourceTableB", sourceDescriptor);
// Using SQL DDL
tableEnv.executeSql("CREATE [TEMPORARY] TABLE MyTable (...) WITH (...)");
```
## 扩展表标识符
表总是通过三元标识符注册，包括catalog名、数据库名和表名。用户可以指定一个catalog和数据库作为当前catalog和当前数据库。有了这些，那么刚刚提到的三元标识符的前两个部分就可以被省略了。如果前两部分的标识符没有指定， 那么会使用当前的catalog和当前数据库。用户也可以通过 Table API或SQL切换当前的catalog和当前的数据库。标识符遵循SQL标准，因此使用时需要用反引号`进行转义。
```java
TableEnvironment tEnv = ...;
tEnv.useCatalog("custom_catalog");
tEnv.useDatabase("custom_database");

Table table = ...;

// register the view named 'exampleView' in the catalog named 'custom_catalog'
// in the database named 'custom_database' 
tableEnv.createTemporaryView("exampleView", table);

// register the view named 'exampleView' in the catalog named 'custom_catalog'
// in the database named 'other_database' 
tableEnv.createTemporaryView("other_database.exampleView", table);

// register the view named 'example.View' in the catalog named 'custom_catalog'
// in the database named 'custom_database' 
tableEnv.createTemporaryView("`example.View`", table);

// register the view named 'exampleView' in the catalog named 'other_catalog'
// in the database named 'other_database' 
tableEnv.createTemporaryView("other_catalog.other_database.exampleView", table);
```
## 查询表
### Table API
Table API是关于Scala/Java的集成语言式查询API。与SQL相反，Table API的查询不是由字符串指定，而是在宿主语言中逐步构建。Table API是基于Table类的，该类表示一个表(流或者批处理)，并提供使用关系操作的方法。这些方法返回一个新的Table对象，该对象表示对输入Table进行关系操作的结果。一些关系操作由多个方法调用组成，例如`table.groupBy(...).select()`，其中`groupBy(...)`指定table的分组，而select(...)在table分组上的投影。文档[Table API](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/table/tableapi/)说明了所有流处理和批处理支持的Table API算子。下面的例子是一个简单的Table API聚合查询的例子:
```java
// get a TableEnvironment
TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section

// register Orders table

// scan registered Orders table
Table orders = tableEnv.from("Orders");
// compute revenue for all customers from France
Table revenue = orders
  .filter($("cCountry").isEqual("FRANCE"))
  .groupBy($("cID"), $("cName"))
  .select($("cID"), $("cName"), $("revenue").sum().as("revSum"));
// emit or convert Table
// execute query
```
### SQL
Flink SQL是基于实现了SQL标准的Apache Calcite，SQL查询由常规字符串指定。文档[SQL]描述了Flink对流处理和批处理表的SQL支持，下面的例子是指定查询并将结果作为Table对象返回。
```java
// get a TableEnvironment
TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
// register Orders table

// compute revenue for all customers from France
Table revenue = tableEnv.sqlQuery(
    "SELECT cID, cName, SUM(revenue) AS revSum " +
    "FROM Orders " +
    "WHERE cCountry = 'FRANCE' " +
    "GROUP BY cID, cName"
  );

// emit or convert Table
// execute query
```
下面的例子指定了一个更新查询，并将查询的结果插入到已注册的表中:
```java
// get a TableEnvironment
TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section

// register "Orders" table
// register "RevenueFrance" output table

// compute revenue for all customers from France and emit to "RevenueFrance"
tableEnv.executeSql(
    "INSERT INTO RevenueFrance " +
    "SELECT cID, cName, SUM(revenue) AS revSum " +
    "FROM Orders " +
    "WHERE cCountry = 'FRANCE' " +
    "GROUP BY cID, cName"
  );
```
## 混用Table API和SQL
Table API和SQL查询的混用非常简单，因为都返回Table对象:
- 可以在SQL查询返回的Table对象上定义Table API查询;
- 在TableEnvironment中注册的结果表可以在SQL查询的FROM子句中引用。通过这种方法就可以在Table API查询的结果上定义SQL查询。

## 输出表
Table通过写入TableSink输出，TableSink是一个通用接口，用于支持多种文件格式(如CSV、Apache Parquet、Apache Avro)、存储系统(如JDBC、Apache HBase、Apache Cassandra、Elasticsearch)或消息队列系统(如Apache Kafka、RibbitMQ)。批处理Table只能写入BatchTableSink，而流处理Table需要指定写入AppendStreamTableSink，RetractStreamTableSink或者UpsertStreamTableSink。请参考文档[Table Sources & Sinks](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/table/sourcessinks/)以获取更多关于可用Sink的信息以及如何自定义DynamicTableSink。方法`Table.insertInto(String tableName)`定义了一个完整的端到端管道将源表中的数据传输到一个被注册的输出表中。该方法通过名称在catalog中查找输出表并确认Table schema和输出表schema一致。可以通过方法`TablePipeline.explain()`和`TablePipeline.execute()`分别来解释和执行一个数据流管道。下面的例子展示如何输出Table:
```java
// get a TableEnvironment
TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section

// create an output Table
final Schema schema = Schema.newBuilder()
    .column("a", DataTypes.INT())
    .column("b", DataTypes.STRING())
    .column("c", DataTypes.BIGINT())
    .build();

tableEnv.createTemporaryTable("CsvSinkTable", TableDescriptor.forConnector("filesystem")
    .schema(schema)
    .option("path", "/path/to/file")
    .format(FormatDescriptor.forFormat("csv")
        .option("field-delimiter", "|")
        .build())
    .build());

// compute a result Table using Table API operators and/or SQL queries
Table result = ...;

// Prepare the insert into pipeline
TablePipeline pipeline = result.insertInto("CsvSinkTable");

// Print explain details
pipeline.printExplain();

// emit the result Table to the registered TableSink
pipeline.execute();
```
## 翻译与执行查询
不论输入数据源是流式的还是批式的，Table API和SQL查询都会被转换成DataStream程序，查询在内部表示为逻辑查询计划并被翻译成2个阶段:
- 优化逻辑执行计划;
- 翻译成DataStream程序;

Table API或者SQL查询在下列情况下会被翻译:
- 当`TableEnvironment.executeSql()`被调用时，该方法是用来执行一个SQL语句，一旦该方法被调用，SQL语句立即被翻译;
- 当`TablePipeline.execute()`被调用时，该方法是用来执行一个源表到输出表的数据流，一旦该方法被调用，TABLE API程序立即被翻译;
- 当`Table.execute()`被调用时，该方法是用来将一个表的内容收集到本地，一旦该方法被调用，TABLE API程序立即被翻译;
- 当`StatementSet.execute()`被调用时，TablePipeline(通过`StatementSet.add()`输出给某个Sink)和INSERT语句(通过调用`StatementSet.addInsertSql()`)会先被缓存到StatementSet中，`StatementSet.execute()`方法被调用时，所有的sink会被优化成一张有向无环图;
- 当Table被转换成DataStream时(参阅[与DataStream集成])。转换完成后，它就称为一个普通的DataStream程序，并会在调用`StreamExecutionEnvironment.execute()`时被执行。
  
## 查询优化
Apache Flink使用并扩展了Apache Calcite来执行复杂的查询优化，这包括一系列基于规则和成本的优化，例如:
- 基于Apache Calcite的子查询解相关;
- 投影剪裁;
- 分区剪裁;
- 过滤器下推;
- 子计划消除重复数据以避免重复计算;
- 特殊子查询重写，包括两部分:
  - 将IN和EXISTS转换为left semi-joins;
  - 将NOT IN和NOT EXISTS转换为left anti-join;
- 可选join重新排序
  - 通过table.optimizer.join-reorder-enabled启用;

**注意**: 当前仅在子查询重写的结合条件下支持IN/EXISTS/NOT IN/NOT EXISTS
优化器不仅基于计划，而且还基于可从数据源获得的丰富统计信息以及每个算子(例如 io，cpu，网络和内存)的细粒度成本来做出明智的决策。高级用户可以通过CalciteConfig对象提供自定义优化，可以通过调用`TableEnvironment＃getConfig＃setPlannerConfig`将其提供给 TableEnvironment。
## 解释表
Table API提供了一种机制来解释计算Table的逻辑和优化查询计划。这是通过`Table.explain()`方法或者`StatementSet.explain()`方法来完成的。`Table.explain()`返回一个Table的计划。`StatementSet.explain()`返回多sink计划的结果。它返回一个描述2种计划的字符串:
- 关系查询的抽象语法树(abstract syntax tree)，即未优化的逻辑查询计划;
- 优化的逻辑查询计划;
- 物理执行计划.

可以用`TableEnvironment.explainSql()`方法和`TableEnvironment.executeSql()`方法支持执行一个EXPLAIN语句获取逻辑和优化查询计划，请参阅[EXPLAIN](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/table/sql/explain/)页面。
下面是一个例子，并且执行`Table.explain()`方法的相应输出:
```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

DataStream<Tuple2<Integer, String>> stream1 = env.fromElements(new Tuple2<>(1, "hello"));
DataStream<Tuple2<Integer, String>> stream2 = env.fromElements(new Tuple2<>(1, "hello"));

// explain Table API
Table table1 = tEnv.fromDataStream(stream1, $("count"), $("word"));
Table table2 = tEnv.fromDataStream(stream2, $("count"), $("word"));
Table table = table1
  .where($("word").like("F%"))
  .unionAll(table2);

System.out.println(table.explain());
```
输出的结果:
```bash
== Abstract Syntax Tree ==
LogicalUnion(all=[true])
:- LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')])
:  +- LogicalTableScan(table=[[Unregistered_DataStream_1]])
+- LogicalTableScan(table=[[Unregistered_DataStream_2]])

== Optimized Physical Plan ==
Union(all=[true], union=[count, word])
:- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')])
:  +- DataStreamScan(table=[[Unregistered_DataStream_1]], fields=[count, word])
+- DataStreamScan(table=[[Unregistered_DataStream_2]], fields=[count, word])

== Optimized Execution Plan ==
Union(all=[true], union=[count, word])
:- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')])
:  +- DataStreamScan(table=[[Unregistered_DataStream_1]], fields=[count, word])
+- DataStreamScan(table=[[Unregistered_DataStream_2]], fields=[count, word])
```
以下代码展示了一个示例以及使用`StatementSet.explain()`的多sink计划的相应输出:
```java
EnvironmentSettings settings = EnvironmentSettings.inStreamingMode();
TableEnvironment tEnv = TableEnvironment.create(settings);

final Schema schema = Schema.newBuilder()
    .column("count", DataTypes.INT())
    .column("word", DataTypes.STRING())
    .build();

tEnv.createTemporaryTable("MySource1", TableDescriptor.forConnector("filesystem")
    .schema(schema)
    .option("path", "/source/path1")
    .format("csv")
    .build());
tEnv.createTemporaryTable("MySource2", TableDescriptor.forConnector("filesystem")
    .schema(schema)
    .option("path", "/source/path2")
    .format("csv")
    .build());
tEnv.createTemporaryTable("MySink1", TableDescriptor.forConnector("filesystem")
    .schema(schema)
    .option("path", "/sink/path1")
    .format("csv")
    .build());
tEnv.createTemporaryTable("MySink2", TableDescriptor.forConnector("filesystem")
    .schema(schema)
    .option("path", "/sink/path2")
    .format("csv")
    .build());

StatementSet stmtSet = tEnv.createStatementSet();

Table table1 = tEnv.from("MySource1").where($("word").like("F%"));
stmtSet.add(table1.insertInto("MySink1"));

Table table2 = table1.unionAll(tEnv.from("MySource2"));
stmtSet.add(table2.insertInto("MySink2"));

String explanation = stmtSet.explain();
System.out.println(explanation);
```
多sink计划的结果是:
```bash
== Abstract Syntax Tree ==
LogicalLegacySink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[count, word])
+- LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')])
   +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]])

LogicalLegacySink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[count, word])
+- LogicalUnion(all=[true])
:- LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')])
:  +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]])
+- LogicalTableScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]])

== Optimized Physical Plan ==
LegacySink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[count, word])
+- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')])
+- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word])

LegacySink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[count, word])
+- Union(all=[true], union=[count, word])
:- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')])
:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word])
+- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word])

== Optimized Execution Plan ==
Calc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')])(reuse_id=[1])
+- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word])

LegacySink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[count, word])
+- Reused(reference_id=[1])

LegacySink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[count, word])
+- Union(all=[true], union=[count, word])
:- Reused(reference_id=[1])
+- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word])
```
# 流式概念
## 时态表(Temporal tables)
时态表是一张随时间变换的表，在Flink中称为动态表，时态表中的每条记录都关联了一个或者多个时间段，所有的Flink表都是时态的(动态的)。时态表包含表的一个或者多个有版本的快照，时态表可以是一张跟踪所有变更记录的表(比如changelog，包含多个表快照)，也可以是物化所有变更之后的表(例如数据库表，只有最新表快照)。
- 版本: 时态表可以划分为一系列带版本的表快照集合，表快照中的版本代表了快照中所有记录的有效区间，有效区间的开始时间和结束时间可以通过用户指定，根据时态表是否可以追踪自身的历史版本与否，时态表可以分为版本表和普通表。
- 版本表: 如果时态表中的记录可以追踪和并访问它的历史版本，这种表我们称之为版本表，来自数据库的changelog可以定义成版本表;
- 普通表: 如果时态表中的记录仅仅可以追踪它的最新版本，这种表我们称之为普通表，来自数据库或HBase的表可以定义成普通表;

### 设计初衷
以订单流关联产品表这个场景举例，orders表包含了来自Kafka的实时订单流，product_changelog表来自数据库表products的changelog, 产品的价格在数据库表 products中是随时间实时变化的。
```sql
SELECT * FROM product_changelog;

(changelog kind)  update_time  product_id product_name price
================= ===========  ========== ============ ===== 
+(INSERT)         00:01:00     p_001      scooter      11.11
+(INSERT)         00:02:00     p_002      basketball   23.11
-(UPDATE_BEFORE)  12:00:00     p_001      scooter      11.11
+(UPDATE_AFTER)   12:00:00     p_001      scooter      12.99
-(UPDATE_BEFORE)  12:00:00     p_002      basketball   23.11 
+(UPDATE_AFTER)   12:00:00     p_002      basketball   19.99
-(DELETE)         18:00:00     p_001      scooter      12.99 
```
表product_changelog表示数据库表products不断增长的changelog, 比如，产品scooter在时间点00:01:00的初始价格是11.11, 在12:00:00的时候涨价到了12.99, 在18:00:00的时候这条产品价格记录被删除。如果我们想输出product_changelog表在10:00:00对应的版本，表的内容如下所示:
```sql
update_time  product_id product_name price
===========  ========== ============ ===== 
00:01:00     p_001      scooter      11.11
00:02:00     p_002      basketball   23.11
```
如果我们想输出product_changelog表在13:00:00对应的版本，表的内容如下所示:
```sql
update_time  product_id product_name price
===========  ========== ============ ===== 
12:00:00     p_001      scooter      12.99
12:00:00     p_002      basketball   19.99
```

如果我们想输出 product_changelog 表在 10:00:00 对应的版本，表的内容如下所示：
```sql
update_time  product_id product_name price
===========  ========== ============ ===== 
12:00:00     p_001      scooter      12.99
12:00:00     p_002      basketball   19.99
```
上述例子中，products表的版本是通过update_time和product_id进行追踪的，product_id对应product_changelog表的主键，update_time对应事件时间。在 Flink中, 这由版本表表示。
#### 关联一张普通表
另一方面，某些用户案列需要连接变化的维表，该表是外部数据库表。假设LatestRates是一个物化的最新汇率表(比如：一张HBase表)，LatestRates总是表示HBase表 Rates的最新内容。我们在10:15:00时查询到的内容如下所示:
```sql
10:15:00 > SELECT * FROM LatestRates;

currency  rate
========= ====
US Dollar 102
Euro      114
Yen       1
```
我们在11:00:00时查询到的内容如下所示:
```sql
11:00:00 > SELECT * FROM LatestRates;

currency  rate
========= ====
US Dollar 102
Euro      116
Yen       1
```
在Flink中, 这由普通表表示。
### 时态表
Flink使用主键约束和事件时间来定义一张版本表和版本视图。
#### 声明普通表
普通表的声明和Flink建表DDL一致，参考create table页面获取更多如何建表的信息。
```sql
-- 用 DDL 定义一张 HBase 表，然后我们可以在 SQL 中将其当作一张时态表使用
-- 'currency' 列是 HBase 表中的 rowKey
 CREATE TABLE LatestRates (   
     currency STRING,   
     fam1 ROW<rate DOUBLE>   
 ) WITH (   
    'connector' = 'hbase-1.4',   
    'table-name' = 'rates',   
    'zookeeper.quorum' = 'localhost:2181'   
 );
```

## 时间属性
Flink可以基于几种不同的*时间*概念来处理数据:
- *处理时间*指的是执行具体操作时的机器时间，大家熟知的绝对时间，例如Java的`System.currentTimeMillis()`;
- *事件时间*指的是数据本身携带的时间，这个时间是在事件产生时的时间;
- *摄入时间*指的是数据进入Flink的时间，在系统内部，会把它当作事件时间来处理

对于时间相关的更多信息。可以参考[事件时间和Watermark](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/concepts/time/)，本页面说明了如何在Flink Table API&SQL里面定义时间以及相关的操作。
### 时间属性介绍
像窗口(在Table API和SQL)这种基于时间的操作，需要有时间信息，因此，Table API中的表就需要提供逻辑时间属性来表示时间，以及支持时间相关的操作。每种类型的表都可以有时间属性，可以在用CREATE TABLE DDL创建表的时候指定，也可以在DataStream中指定，也可以在定义TableSource时指定。一旦时间属性定义好，它就可以像普通列一样使用，也可以在时间相关的操作中使用。只要时间属性没有被修改，而是简单地从一个表传递到另一个表，它就仍然是一个有效的时间属性。时间属性可以像普通的时间戳的列一样被使用和计算。一旦时间属性被用在了计算中，它就会被物化，进而变成一个普通的时间戳。普通的时间戳是无法跟Flink的时间以及watermark等一起使用的，所以普通的时间戳就无法用在时间相关的操作中。
Table API程序需要在streaming environment中指定时间属性:
```java
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); // default
// 或者:
// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);
// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
```
### 处理时间
处理时间是基于机器的本地时间来处理数据，它是最简单的一种时间概念，但是它不能提供确定性，它既不需要从数据里获取时间也不需要生成watermark。共有3种方法可以定义处理时间。
#### 在创建表的DDL中定义
处理时间属性可以在创建表的DDL中用计算列的方式定义，用`PROCTIME()`就可以定义处理时间，函数`PROCTIME()`的返回类型是`TIMESTAMP_LTZ`。关于计算列，更多的信息可以参考:[CREATE TABLE DDL](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/table/sql/create/#create-table)。
```sql
CREATE TABLE user_actions (
  user_name STRING,
  data STRING,
  user_action_time AS PROCTIME() -- 声明一个额外的列作为处理时间属性
) WITH (
  ...
);

SELECT TUMBLE_START(user_action_time, INTERVAL '10' MINUTE), COUNT(DISTINCT user_name)
FROM user_actions
GROUP BY TUMBLE(user_action_time, INTERVAL '10' MINUTE);
```
#### 从DataStream到Table转换时定义
处理时间属性可以在schema定义的时候用.proctime后缀来定义。时间属性一定不能定义在一个已有字段上，所以它只能定义在schema定义的最后。
```java
DataStream<Tuple2<String, String>> stream = ...;

// 声明一个额外的字段作为时间属性字段
Table table = tEnv.fromDataStream(stream, $("user_name"), $("data"), $("user_action_time").proctime());

WindowedTable windowedTable = table.window(
        Tumble.over(lit(10).minutes())
            .on($("user_action_time"))
            .as("userActionWindow"));
```
#### 使用TableSource定义
处理时间属性可以在实现了DefinedProctimeAttribute接口的TableSource中定义。逻辑的时间属性会放在TableSource已有物理字段的最后.
```java
// 定义一个由处理时间属性的 table source
public class UserActionSource implements StreamTableSource<Row>, DefinedProctimeAttribute {

	@Override
	public TypeInformation<Row> getReturnType() {
		String[] names = new String[] {"user_name" , "data"};
		TypeInformation[] types = new TypeInformation[] {Types.STRING(), Types.STRING()};
		return Types.ROW(names, types);
	}

	@Override
	public DataStream<Row> getDataStream(StreamExecutionEnvironment execEnv) {
		// create stream
		DataStream<Row> stream = ...;
		return stream;
	}

	@Override
	public String getProctimeAttribute() {
		// 这个名字的列会被追加到最后，作为第三列
		return "user_action_time";
	}
}

// register table source
tEnv.registerTableSource("user_actions", new UserActionSource());

WindowedTable windowedTable = tEnv
	.from("user_actions")
	.window(Tumble
	    .over(lit(10).minutes())
	    .on($("user_action_time"))
	    .as("userActionWindow"));
```
### 事件时间
事件时间允许程序按照数据中包含的时间来处理，这样可以在有乱序或者晚到数据的情况下产生一致的处理结果，它可以保证从外部存储读取数据后产生可以复现(replayable)的结果。除此之外，事件时间可以让程序在流式和批式作业中使用同样的语法，在流式程序中的事件时间属性，在批式程序中就是一个正常的时间字段。为了能够处理乱序的事件，并且区分正常达到和晚到的事件，Flink需要从事件中获取事件时间并且产生watermark。事件时间属性也有类似于处理时间的3种定义方式。
#### DDL中定义
事件时间属性可以用WATERMARK语句在CREATE TABLE DDL中进行定义，WATERMARK语句在一个已有字段上定义一个watermark生成表达式，同时标记这个已有字段为时间属性字段，更多信息可以参考: [CREATE TABLE DDL](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/table/sql/create/#create-table)，Flink支持和在TIMESTAMP/TIMESTAM_LTZ列上定义事件时间。如果源数据中的时间戳数据表示为年-月-日-时-分-秒，则通常为不带时区信息的字符串值，例如2020-04-15 20:13:40.564，建议将事件时间属性定义在TIMESTAMP列上:
```sql

CREATE TABLE user_actions (
  user_name STRING,
  data STRING,
  user_action_time TIMESTAMP(3),
  -- 声明 user_action_time 是事件时间属性，并且用 延迟 5 秒的策略来生成 watermark
  WATERMARK FOR user_action_time AS user_action_time - INTERVAL '5' SECOND
) WITH (
  ...
);

SELECT TUMBLE_START(user_action_time, INTERVAL '10' MINUTE), COUNT(DISTINCT user_name)
FROM user_actions
GROUP BY TUMBLE(user_action_time, INTERVAL '10' MINUTE);
```
源数据中的时间戳数据表示为一个纪元(epoch)时间，通常是一个long值，例如1618989564564，建议将事件时间属性定义在TIMESTAMP_LTZ列上:
```sql
CREATE TABLE user_actions (
 user_name STRING,
 data STRING,
 ts BIGINT,
 time_ltz AS TO_TIMESTAMP_LTZ(ts, 3),
 -- declare time_ltz as event time attribute and use 5 seconds delayed watermark strategy
 WATERMARK FOR time_ltz AS time_ltz - INTERVAL '5' SECOND
) WITH (
 ...
);

SELECT TUMBLE_START(time_ltz, INTERVAL '10' MINUTE), COUNT(DISTINCT user_name)
FROM user_actions
GROUP BY TUMBLE(time_ltz, INTERVAL '10' MINUTE);
```
#### 从DataStream到Table转换时定义
事件时间属性可以用.rowtime后缀在定义DataStream schema的时候定义。时间戳和watermark在这之前一定是在DataStream上已经定义好了。在从DataStream转换到Table时，由于DataStream没有时区概念，因此Flink总是将rowtime属性解析形成TIMESTAMP WITHOUT ZONe类型，并且将所有事件时间的值都视为UTC时区的值。在从DataStream到Table转换时定义事件时间属性有2种方式。取决于用.rowtime后缀修饰的字段名称是否时已有字段，事件时间字段可以是:
- 在schema的结尾追加一个新的字段;
- 替换一个已经存在的字段;
不管在哪种情况下，事件时间字段都表示DataStream中定义的事件的时间戳。
```java
// Option 1:

// 基于 stream 中的事件产生时间戳和 watermark
DataStream<Tuple2<String, String>> stream = inputStream.assignTimestampsAndWatermarks(...);

// 声明一个额外的逻辑字段作为事件时间属性
Table table = tEnv.fromDataStream(stream, $("user_name"), $("data"), $("user_action_time").rowtime());


// Option 2:

// 从第一个字段获取事件时间，并且产生 watermark
DataStream<Tuple3<Long, String, String>> stream = inputStream.assignTimestampsAndWatermarks(...);

// 第一个字段已经用作事件时间抽取了，不用再用一个新字段来表示事件时间了
Table table = tEnv.fromDataStream(stream, $("user_action_time").rowtime(), $("user_name"), $("data"));

// Usage:

WindowedTable windowedTable = table.window(Tumble
       .over(lit(10).minutes())
       .on($("user_action_time"))
       .as("userActionWindow"));
```
#### 使用TableSource定义
事件时间属性可以在实现了DefinedRowTimeAttributes的TableSource中定义。getRowtimeAttributeDescriptors()方法返回RowtimeAttributeDescriptor的列表，包含了描述事件时间属性的字段名字、如何计算事件时间、以及watermark生成策略等信息。同时需要确保getDataStream 返回的DataStream已经定义好了时间属性。只有在定义了StreamRecordTimestamp时间戳分配器的时候，才认为DataStream是有时间戳信息的。只有定义了PreserveWatermarks watermark生成策略的DataStream的watermark才会被保留。反之，则只有时间字段的值是生效的。
```java
// 定义一个有事件时间属性的 table source
public class UserActionSource implements StreamTableSource<Row>, DefinedRowtimeAttributes {

	@Override
	public TypeInformation<Row> getReturnType() {
		String[] names = new String[] {"user_name", "data", "user_action_time"};
		TypeInformation[] types =
		    new TypeInformation[] {Types.STRING(), Types.STRING(), Types.LONG()};
		return Types.ROW(names, types);
	}

	@Override
	public DataStream<Row> getDataStream(StreamExecutionEnvironment execEnv) {
		// 构造 DataStream
		// ...
		// 基于 "user_action_time" 定义 watermark
		DataStream<Row> stream = inputStream.assignTimestampsAndWatermarks(...);
		return stream;
	}

	@Override
	public List<RowtimeAttributeDescriptor> getRowtimeAttributeDescriptors() {
		// 标记 "user_action_time" 字段是事件时间字段
		// 给 "user_action_time" 构造一个时间属性描述符
		RowtimeAttributeDescriptor rowtimeAttrDescr = new RowtimeAttributeDescriptor(
			"user_action_time",
			new ExistingField("user_action_time"),
			new AscendingTimestamps());
		List<RowtimeAttributeDescriptor> listRowtimeAttrDescr = Collections.singletonList(rowtimeAttrDescr);
		return listRowtimeAttrDescr;
	}
}

// register the table source
tEnv.registerTableSource("user_actions", new UserActionSource());

WindowedTable windowedTable = tEnv
	.from("user_actions")
	.window(Tumble.over(lit(10).minutes()).on($("user_action_time")).as("userActionWindow"));
```