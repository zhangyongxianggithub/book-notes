[TOC]
如何通过流行的构建工具配置项目，必要的依赖项以及覆盖高级配置。每个Flink应用程序都依赖于一组Flink库，至少依赖Flink API，还依赖某些连接器库以及用户开发的自定义的数据处理逻辑所需要的第三方依赖。
## 开始
使用下面的命令脚本模板来创建Flink项目:
```bash
$ mvn archetype:generate                \
  -DarchetypeGroupId=org.apache.flink   \
  -DarchetypeArtifactId=flink-quickstart-java \
  -DarchetypeVersion=1.16.0
$ curl https://flink.apache.org/q/quickstart.sh | bash -s 1.16.0
```
Gradle的方式请参考官方文档。
## 需要哪些依赖项
通常需要的依赖项:
- Flink API，用来开发你的作业;
- 连接器和格式，以将你的作业与外部系统集成;
- 测试实用程序，测试你的作业
### Flink API
2大API:
- DataStream API;
- Table API/SQL;
可以单独使用或者混合使用。

| **你要使用的 API**                  | **你需要添加的依赖项**                     |
|--------------------------------|-----------------------------------|
| DataStream                     | flink-streaming-java              |
| DataStream Scala 版             | flink-streaming-scala_2.12        |
| Table API                      | flink-table-api-java              |
| Table API Scala 版              | flink-table-api-scala_2.12        |
| Table API + DataStream         | flink-table-api-java-bridge       |
| Table API + DataStream Scala 版 | flink-table-api-scala-bridge_2.12 |

## 运行和打包
如果你想通过简单的执行主类来运行你的作业，需要classpath里有flink-runtime。对于Table API程序，你还需要flink-table-runtime和flink-table-planner-loader。根据经验，我们建议将应用程序代码及其所有需要的依赖项打包进一个fat/uber JAR中，这包括打包你作业用到的连接器、格式和第三方依赖项。此规则不适用于Java API、DataStream Scala API以及前面提到的运行时模块，它们已经由Flink本身提供，不应包含在作业的uber JAR中。你可以把该作业JAR提交到已经运行的Flink集群，也可以轻松将其添加到Flink应用程序容器镜像中，而无需修改发行版。
# 使用Maven
Flink程序以来的库:
- Flink API库
- Flink连接器库
- 用户逻辑的第三方库
使用maven创建一个Flink应用程序

```bash
mvn archetype:generate               
  -DarchetypeGroupId=org.apache.flink   \
  -DarchetypeArtifactId=flink-quickstart-java \
  -DarchetypeVersion=1.17-SNAPSHOT
```

如果你想通过简单地执行主类来运行你的作业，需要classpath里有flink-runtime。对于Table API程序，还需要flink-table-runtime和 flink-table-planner-loader。Flink建议将应用程序代码与所有必须的依赖打包进一个fat/uber JAR中，包括打包连接器与第三方依赖，此规则不适用于 Java API、DataStream Scala API 以及前面提到的运行时模块，它们已经由 Flink 本身提供，不应包含在作业的 uber JAR 中。你可以把该作业 JAR 提交到已经运行的 Flink 集群，也可以轻松将其添加到 Flink 应用程序容器镜像中，而无需修改发行版。
# 如何使用Maven配置项目
本指南将向您展示如何使用Maven配置Flink作业项目，Maven是由Apache Software Foundation开源的自动化构建工具，使您能够构建、发布和部署项目。您可以使用它来管理软件项目的整个生命周期。
## 要求
- maven 3.0.4+
- java 11+
## 将项目导入IDE
Java的默认JVM堆大小对于Flink来说可能太小，您应该手动增加它。在Eclipse中，选中Run Configurations -> Arguments并在VM Arguments框里填上：-Xmx800m。在IntelliJ IDEA中，推荐选中Help | Edit Custom VM Options菜单修改JVM属性。要使应用程序在IntelliJ IDEA中运行，需要在运行配置中的Include dependencies with Provided scope打勾。如果此选项不可用（可能是由于使用了较旧的IntelliJ IDEA版本），可创建一个调用应用程序main()方法的测试用例。
## 构建项目
如果您想构建/打包您的项目，请转到您的项目目录并运行`mvn clean package`命令。您将找到一个JAR文件，其中包含您的应用程序（还有已作为依赖项添加到应用程序的连接器和库）: target/<artifact-id>-<version>.jar。
注意: 如果您使用不同于DataStreamJob的类作为应用程序的主类/入口点，我们建议您对pom.xml文件里的mainClassName配置进行相应的修改。这样，Flink 可以通过JAR文件运行应用程序，而无需额外指定主类.
## 向项目添加依赖项
比如按照下面的方式添加依赖项
```xml
<dependencies>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-kafka</artifactId>
        <version>1.17-SNAPSHOT</version>
    </dependency>
</dependencies>
```
当您在由Java Project Template、Scala Project Template或Gradle创建出来的项目里，运行mvn clean package会自动将应用程序依赖打包进应用程序JAR。对于不是通过这些模板创建的项目，我们建议使用Maven Shade插件以将所有必需的依赖项打包进应用程序JAR。重要提示: 请注意，应将所有核心依赖项的生效范围置为provided。这意味着只需要对它们进行编译，但不应将它们打包进项目生成的应用程序JAR文件中。如果不设置为provided，最好的情况是生成的JAR变得过大，因为它还包含所有Flink核心依赖项。最坏的情况是添加到应用程序JAR文件中的Flink核心依赖项与您自己的一些依赖项的版本冲突（通常通过反向类加载来避免）。要将依赖项正确地打包进应用程序JAR中，必须把应用程序依赖项的生效范围设置为compile。
## 打包应用程序
在部署应用到Flink环境之前，您需要根据使用场景用不同的方式打包Flink应用程序。如果您想为Flink作业创建JAR并且只使用Flink依赖而不使用任何第三方依赖，就不需要创建一个uber/fatJAR或将任何依赖打进包。如果您想为Flink作业创建JAR并使用未内置在Flink发行版中的外部依赖项，您可以将它们添加到发行版的类路径中，或者将它们打包进您的uber/fat应用程序JAR中。您可以将生成的uber/fat JAR提交到本地或远程集群：
```bash
bin/flink run -c org.example.MyJob myFatJar.jar
```
## 创建包含依赖项的 uber/fat JAR 的模板
为构建一个包含所有必需的Connector第三方类库依赖的应用程序JAR，可以使用如下shade插件定义：
```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>3.1.1</version>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <artifactSet>
                            <excludes>
                                <exclude>com.google.code.findbugs:jsr305</exclude>
                            </excludes>
                        </artifactSet>
                        <filters>
                            <filter>
                                <!-- Do not copy the signatures in the META-INF folder.
                                Otherwise, this might cause SecurityExceptions when using the JAR. -->
                                <artifact>*:*</artifact>
                                <excludes>
                                    <exclude>META-INF/*.SF</exclude>
                                    <exclude>META-INF/*.DSA</exclude>
                                    <exclude>META-INF/*.RSA</exclude>
                                </excludes>
                            </filter>
                        </filters>
                        <transformers>
                            <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <!-- Replace this with the main class of your job -->
                                <mainClass>my.programs.main.clazz</mainClass>
                            </transformer>
                            <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
```
Maven shade插件默认会包含生效范围是runtime或compile的依赖项。
# 如何使用Gradle配置您的项目
# 连接器和格式
Flink应用程序可以通过连接器哎来读取和写入各种外部系统，支持多种格式，以便对数据进行编码和解码以匹配Flink数据结构。
## 可用的组件
为了使用连接器与格式，您需要确保Flink可以访问实现了这些功能的组件。对于Flink社区支持的每个连接器，我们在 Maven Central发布了两类组件:
- flink-connector-<NAME>这是一个精简JAR，仅包括连接器代码，但不包括最终的第三方依赖项;
- flink-sql-connector-<NAME>这是一个包含连接器第三方依赖项的uber JAR;
同样适用于格式，请注意，某些连接器可能没有相应的flink-sql-connector-<NAME>组件，因为它们不需要第三方依赖项。uber/fat JAR主要与SQL客户端一起使用，但您也可以在任何DataStream/Table应用程序中使用它们。
## 使用组件
为了使用连接器/格式模块，您可以:
- 把精简JAR及其传递依赖项打包进您的作业JAR;
- 把uber JAR打包进您的作业JAR;
- 把uber JAR直接复制到Flink发行版的/lib文件夹内;

关于打包依赖项，请查看[Maven](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/configuration/maven/)和[Gradle](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/configuration/gradle/)指南。有关Flink发行版的参考，请查看[Flink依赖剖析](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/configuration/overview/#Flink-%e4%be%9d%e8%b5%96%e5%89%96%e6%9e%90)。
决定是打成uber JAR、精简JAR还是仅在发行版包含依赖项取决于您和您的使用场景。如果您使用uber JAR，您将对作业里的依赖项版本有更多的控制权；如果您使用精简JAR，由于您可以在不更改连接器版本的情况下更改版本（允许二进制兼容），您将对传递依赖项有更多的控制权；如果您直接在Flink发行版的/lib目录里内嵌连接器uber JAR，您将能够在一处控制所有作业的连接器版本。
# 用于测试的依赖项
Flink提供了用于测试作业的实用程序，可以将其添加为依赖项
## DataStream API测试
为使用DataStream API构建的作业开发测试用例，需要添加下面的依赖项:
```xml

<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-test-utils</artifactId>
    <version>1.16.0</version>
    <scope>test</scope>
</dependency>
```
在测试用例中，该模块提供了MiniCluster(一个可配置的轻量级Flink集群，能在JUnit测试中运行)，可以直接执行作业。更多细节[DataStream API测试](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/datastream/testing/)
## Table API测试
在IDE本地中测试Table API/SQL程序，除了flink-test-utils之外，还需要添加:
```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-test-utils</artifactId>
    <version>1.16.0</version>
    <scope>test</scope>
</dependency>
```
自动引入查询计划器和运行时，分别用于计划和执行查询。
# 高级配置
## Flink依赖剖析
Flink自身由一组类和依赖项组成。这些共同构成了Flink运行时的核心。在Flink应用程序启动时必须存在，会提供诸如通信协调、网络管理、检查点、容错、API、算子(窗口)、资源管理等领域的服务。这些核心类和依赖项都打包在flink-dist.jar中，可以在lib中找到，也是flink容器镜像的基础部分。您可以将其近似地看作是包含String和List等公用类的Java核心库。为了保持核心依赖项尽可能小并避免依赖冲突，Flink Core Dependencies不包含任何连接器或库（如CEP、SQL、ML），以避免在类路径中有过多的类和依赖项。Flink发行版/lib目录里还有包括常用模块在内的各种JAR文件，例如执行 Table作业的必需模块、一组连接器和format。默认情况下会自动加载，若要禁止加载只需将它们从classpath中的/lib目录中删除即可。Flink还在/opt文件夹下提供了额外的可选依赖项，可以通过移动这些JAR文件到/lib目录来启用这些依赖项。有关类加载的更多细节，请查阅[Flink 类加载](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/ops/debugging/debugging_classloading/)。
## Scala版本
不同的Scala版本二进制不兼容，所有（传递地）依赖于Scala的Flink依赖项都以它们构建的Scala版本为后缀（如 flink-streaming-scala_2.12）。如果您只使用Flink的Java API，您可以使用任何Scala版本。如果您使用 Flink的Scala API，则需要选择与应用程序的Scala匹配的Scala版本。有关如何为特定Scala版本构建Flink的细节，请查阅[构建指南](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/flinkdev/building/#scala-versions)。2.12.8之后的Scala版本与之前的2.12.x版本二进制不兼容，使Flink项目无法将其2.12.x版本直接升级到2.12.8以上。您可以按照构建指南在本地为更高版本的Scala构建Flink。为此，您需要在构建时添加-Djapicmp.skip以跳过二进制兼容性检查。有关更多细节，请查阅Scala 2.12.8版本说明。相关部分指出：第二项修改是二进制不兼容的：2.12.8编译器忽略了由更早版本的2.12编译器生成的某些方法。然而我们相信这些方法永远不会被使用，现有的编译代码仍可工作。有关更多详细信息，请查阅pull request描述。
## Table依赖剖析
Flink发行版默认包含执行Flink SQL任务的必要的JAR文件，主要有:
- flink-table-api-java-uber-1.16.0.jar 包含所有的Java API;
- flink-table-runtime-1.16.0.jar 包含Table运行时;
- flink-table-planner-loader-1.16.0.jar 包含查询计划器
以前这些JAR都打包进了flink-table.jar中，从flink1.15开始，已将其划分成3个JAR，以允许用户使用`flink-table-planner-loader-1.16.0.jar`充当`flink-table-planner_2.12-1.16.0.jar`。虽然Table Java API内置于发行版中，但默认情况下不包含Table Scala API。在 Flink Scala API中使用格式和连接器时，您需要手动下载这些JAR包并将其放到发行版的/lib文件夹中（推荐），或者将它们打包为Flink SQL作业的 uber/fat JAR包中的依赖项。有关更多细节，请查阅如何[连接外部系统](https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/connectors/table/overview/)。
### Table Planner和Table Planner加载器
从Flink 1.15开始，发行版包含两个planner:
- `flink-table-planner_2.12-1.16.0.jar`, 位于/opt目录, 包含查询计划器;
- `flink-table-planner-loader-1.16.0.jar`, 位于/lib目录默认被加载, 包含隐藏在单独的classpath里的查询计划器 (您无法直接使用io.apache.flink.table.planner包)。
这两个planner JAR文件的代码功能相同，但打包方式不同。若使用第一个文件，您必须使用与其相同版本的Scala；若使用第二个，由于Scala已经被打包进该文件里，您不需要考虑Scala版本问题。默认情况下，发行版使用 flink-table-planner-loader。如果想使用内部查询计划器，您可以换掉 JAR包（拷贝flink-table-planner_2.12.jar并复制到发行版的/lib目录）。请注意，此时会被限制用于Flink发行版的Scala版本。这两个 planner无法同时存在于classpath，如果您在/lib目录同时加载他们，Table任务将会失败。在即将发布的Flink版本中，我们将停止在Flink发行版中发布flink-table-planner_2.12组件。我们强烈建议迁移您的作业/自定义连接器/格式以使用前述API模块，而不依赖此内部planner。如果您需要 planner中尚未被API块暴露的一些功能，请与社区讨论。
## Hadoop依赖
**一般规则**: 没有必要直接添加Hadoop依赖到您的应用程序里，唯一的例外是您通过[Hadoop兼容](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/dataset/hadoop_compatibility/)使用已有的Hadoop读写format。如果您想将Flink与Hadoop一起使用，您需要有一个包含Hadoop依赖项的Flink系统，而不是添加Hadoop作为应用程序依赖项。换句话说，Hadoop必须是Flink系统本身的依赖，而不是用户代码的依赖。Flink将使用HADOOP_CLASSPATH环境变量指定Hadoop依赖项，可以这样设置:
```shell
export HADOOP_CLASSPATH=`hadoop classpath`
```
这样设计有两个主要原因: 
- 一些Hadoop交互可能在用户应用程序启动之前就发生在Flink内核。其中包括为检查点配置HDFS、通过Hadoop的Kerberos令牌进行身份验证或在YARN 上部署;
- Flink的反向类加载方式在核心依赖项中隐藏了许多传递依赖项。这不仅适用于Flink自己的核心依赖项，也适用于已有的Hadoop依赖项。这样，应用程序可以使用相同依赖项的不同版本，而不会遇到依赖项冲突。当依赖树变得非常大时，这非常有用。

如果您在IDE内开发或测试期间需要Hadoop依赖项（比如用于HDFS访问），应该限定这些依赖项的使用范围（如test或provided）。


